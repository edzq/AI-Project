03/23/2022 16:35:32 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='scibert-scivocab-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
03/23/2022 16:36:45 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='scibert-scivocab-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
03/23/2022 16:36:50 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmpjkl0wq1u
03/23/2022 16:36:58 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar from cache at /tmp/tmpjkl0wq1u/scibert_scivocab_cased/vocab.txt
03/23/2022 16:36:58 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmp3y86pfd8
03/23/2022 16:37:06 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31116
}

03/23/2022 16:37:09 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
03/23/2022 16:37:09 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
03/23/2022 16:37:09 - INFO - root -   train data size: 157770, validation data size: 0
03/23/2022 16:37:13 - INFO - root -   Number of train optimization steps is : 29580
03/23/2022 17:35:57 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='scibert-scivocab-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
03/23/2022 17:36:01 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmp51jy3pxf
03/23/2022 17:36:09 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar from cache at /tmp/tmp51jy3pxf/scibert_scivocab_cased/vocab.txt
03/23/2022 17:36:09 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmpd3u9vyqv
03/23/2022 17:36:17 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31116
}

03/23/2022 17:36:20 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
03/23/2022 17:36:20 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
03/23/2022 17:36:20 - INFO - root -   train data size: 157770, validation data size: 0
03/23/2022 17:36:25 - INFO - root -   Number of train optimization steps is : 29580
03/23/2022 19:12:01 - INFO - root -   Epoch 1, Train loss : 0.0059
03/23/2022 20:46:48 - INFO - root -   Epoch 2, Train loss : 0.0023
03/23/2022 22:21:39 - INFO - root -   Epoch 3, Train loss : 0.0014
03/23/2022 23:30:43 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='scibert-scivocab-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
03/23/2022 23:30:44 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmpaon_xo61
03/23/2022 23:30:53 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar from cache at /tmp/tmpaon_xo61/scibert_scivocab_cased/vocab.txt
03/23/2022 23:30:53 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmphdlq9ya5
03/23/2022 23:31:02 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31116
}

03/23/2022 23:31:05 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
03/23/2022 23:31:05 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
03/23/2022 23:31:05 - INFO - root -   train data size: 100, validation data size: 0
03/23/2022 23:31:11 - INFO - root -   Number of train optimization steps is : 18
03/23/2022 23:31:15 - INFO - root -   Epoch 1, Train loss : 0.1681
03/23/2022 23:31:20 - INFO - root -   Epoch 2, Train loss : 0.0421
03/23/2022 23:31:24 - INFO - root -   Epoch 3, Train loss : 0.0180
03/23/2022 23:41:48 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='scibert-scivocab-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
03/23/2022 23:41:53 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmp793bu8h5
03/23/2022 23:42:02 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar from cache at /tmp/tmp793bu8h5/scibert_scivocab_cased/vocab.txt
03/23/2022 23:42:02 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmp9mzfp4tb
03/23/2022 23:42:12 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31116
}

03/23/2022 23:42:15 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
03/23/2022 23:42:15 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
03/23/2022 23:42:15 - INFO - root -   train data size: 157770, validation data size: 0
03/23/2022 23:42:21 - INFO - root -   Number of train optimization steps is : 29580
03/24/2022 01:07:18 - INFO - root -   Epoch 1, Train loss : 0.0059
03/24/2022 01:57:20 - INFO - root -   Epoch 2, Train loss : 0.0023
03/24/2022 02:46:37 - INFO - root -   Epoch 3, Train loss : 0.0014
03/24/2022 04:48:27 - INFO - root -   Loading model:
BertTokenClassifier(bert_config_json={'attention_probs_dropout_prob': 0.1,
                                      'hidden_act': 'gelu',
                                      'hidden_dropout_prob': 0.1,
                                      'hidden_size': 768,
                                      'initializer_range': 0.02,
                                      'intermediate_size': 3072,
                                      'layer_norm_eps': 1e-12,
                                      'max_position_embeddings': 512,
                                      'num_attention_heads': 12,
                                      'num_hidden_layers': 12,
                                      'type_vocab_size': 2,
                                      'vocab_size': 31116},
                    bert_model='scibert-sc...
                                            ('[unused21]', 21),
                                            ('[unused22]', 22),
                                            ('[unused23]', 23),
                                            ('[unused24]', 24),
                                            ('[unused25]', 25),
                                            ('[unused26]', 26),
                                            ('[unused27]', 27),
                                            ('[unused28]', 28),
                                            ('[unused29]', 29), ...]),
                    do_lower_case=False, eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
03/24/2022 04:51:15 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31116
}

03/24/2022 04:51:17 - INFO - root -   Loading model:
BertTokenClassifier(bert_config_json={'attention_probs_dropout_prob': 0.1,
                                      'hidden_act': 'gelu',
                                      'hidden_dropout_prob': 0.1,
                                      'hidden_size': 768,
                                      'initializer_range': 0.02,
                                      'intermediate_size': 3072,
                                      'layer_norm_eps': 1e-12,
                                      'max_position_embeddings': 512,
                                      'num_attention_heads': 12,
                                      'num_hidden_layers': 12,
                                      'type_vocab_size': 2,
                                      'vocab_size': 31116},
                    bert_model='scibert-sc...
                                            ('[unused21]', 21),
                                            ('[unused22]', 22),
                                            ('[unused23]', 23),
                                            ('[unused24]', 24),
                                            ('[unused25]', 25),
                                            ('[unused26]', 26),
                                            ('[unused27]', 27),
                                            ('[unused28]', 28),
                                            ('[unused29]', 29), ...]),
                    do_lower_case=False, eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
04/30/2022 21:54:45 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
04/30/2022 21:57:15 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache, downloading to /tmp/tmp60ahqscq
04/30/2022 21:57:15 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   copying /tmp/tmp60ahqscq to cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 21:57:15 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   creating metadata file for /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 21:57:15 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   removing temp file /tmp/tmp60ahqscq
04/30/2022 21:57:15 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 21:57:15 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache, downloading to /tmp/tmptxr4pz17
04/30/2022 21:57:22 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   copying /tmp/tmptxr4pz17 to cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 21:57:24 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   creating metadata file for /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 21:57:24 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   removing temp file /tmp/tmptxr4pz17
04/30/2022 21:57:24 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache, downloading to /tmp/tmppgfxc06j
04/30/2022 21:57:24 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   copying /tmp/tmppgfxc06j to cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 21:57:24 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   creating metadata file for /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 21:57:24 - INFO - bert_sklearn.model.pytorch_pretrained.file_utils -   removing temp file /tmp/tmppgfxc06j
04/30/2022 21:57:24 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 21:57:24 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 21:57:24 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 21:57:27 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 21:57:27 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 21:57:27 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:04:14 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
04/30/2022 22:04:15 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:04:15 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:04:15 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:04:15 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:04:18 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:04:18 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:04:18 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:11:17 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:11:17 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:11:17 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:11:17 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:11:19 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:11:19 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:11:19 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:15:32 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:15:32 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:15:32 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:15:32 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:15:35 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:15:35 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:15:35 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:17:33 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=8,
                    validation_fraction=0.0)
04/30/2022 22:17:40 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:17:40 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:17:40 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:17:40 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:17:43 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:17:43 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:17:43 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:17:59 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=4,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=4,
                    validation_fraction=0.0)
04/30/2022 22:18:03 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:18:03 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:18:03 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:18:03 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:18:06 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:18:06 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:18:06 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:28:26 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
04/30/2022 22:28:27 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:28:27 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:28:27 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:28:27 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:28:30 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:28:30 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:28:30 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:28:57 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:28:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:28:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:28:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:28:59 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:28:59 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:28:59 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:29:24 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:29:24 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:29:24 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:29:24 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:29:26 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:29:26 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:29:26 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:37:50 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:37:50 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:37:50 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:37:50 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:37:53 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:37:53 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:37:53 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:39:14 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:39:14 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:39:14 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:39:14 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:39:17 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:39:17 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:39:17 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:42:14 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=4,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=4,
                    validation_fraction=0.0)
04/30/2022 22:42:15 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:42:16 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:42:16 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:42:16 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:42:18 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:42:18 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:42:18 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 22:57:53 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
04/30/2022 22:57:54 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 22:57:55 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 22:57:55 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 22:57:55 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 22:57:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 22:57:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 22:57:57 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 23:11:13 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
04/30/2022 23:11:15 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 23:11:15 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 23:11:15 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 23:11:15 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 23:11:17 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 23:11:17 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 23:11:17 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 23:13:31 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 23:13:31 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 23:13:31 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 23:13:31 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 23:13:34 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 23:13:34 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 23:13:34 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 23:29:46 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
04/30/2022 23:29:47 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
04/30/2022 23:29:48 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
04/30/2022 23:29:48 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
04/30/2022 23:29:48 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

04/30/2022 23:29:50 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
04/30/2022 23:29:50 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/30/2022 23:29:50 - INFO - root -   train data size: 42388, validation data size: 0
04/30/2022 23:29:55 - INFO - root -   Number of train optimization steps is : 7947
04/30/2022 23:43:03 - INFO - root -   Epoch 1, Train loss : 0.0099
04/30/2022 23:56:12 - INFO - root -   Epoch 2, Train loss : 0.0023
05/01/2022 00:09:23 - INFO - root -   Epoch 3, Train loss : 0.0014
05/01/2022 20:59:38 - INFO - root -   Loading model:
BertTokenClassifier(bert_config_json={'architectures': ['BertForMaskedLM'],
                                      'attention_probs_dropout_prob': 0.1,
                                      'hidden_act': 'gelu',
                                      'hidden_dropout_prob': 0.1,
                                      'hidden_size': 768,
                                      'initializer_range': 0.02,
                                      'intermediate_size': 3072,
                                      'layer_norm_eps': 1e-12,
                                      'max_position_embeddings': 512,
                                      'model_type': 'bert',
                                      'num_attention_heads': 12,
                                      'num_hidden_layers': 12,
                                      'pad_t...
                                            ('[unused21]', 21),
                                            ('[unused22]', 22),
                                            ('[unused23]', 23),
                                            ('[unused24]', 24),
                                            ('[unused25]', 25),
                                            ('[unused26]', 26),
                                            ('[unused27]', 27),
                                            ('[unused28]', 28),
                                            ('[unused29]', 29), ...]),
                    do_lower_case=False, eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
05/01/2022 21:31:43 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
05/01/2022 21:36:54 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
05/01/2022 21:36:54 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
05/01/2022 21:36:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
05/01/2022 21:36:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
05/01/2022 21:36:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

05/01/2022 21:36:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 21:36:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/01/2022 21:36:57 - INFO - root -   train data size: 1000, validation data size: 0
05/01/2022 21:37:00 - INFO - root -   Number of train optimization steps is : 186
05/01/2022 21:37:18 - INFO - root -   Epoch 1, Train loss : 0.1409
05/01/2022 21:37:36 - INFO - root -   Epoch 2, Train loss : 0.0534
05/01/2022 21:37:54 - INFO - root -   Epoch 3, Train loss : 0.0533
05/01/2022 21:49:51 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='bert-base-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
05/01/2022 21:49:51 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
05/01/2022 21:49:51 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
05/01/2022 21:49:51 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
05/01/2022 21:49:51 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

05/01/2022 21:49:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 21:49:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/01/2022 21:49:54 - INFO - root -   train data size: 1000, validation data size: 0
05/01/2022 21:49:58 - INFO - root -   Number of train optimization steps is : 186
05/01/2022 21:50:16 - INFO - root -   Epoch 1, Train loss : 0.1409
05/01/2022 21:50:33 - INFO - root -   Epoch 2, Train loss : 0.0534
05/01/2022 21:50:51 - INFO - root -   Epoch 3, Train loss : 0.0533
05/01/2022 21:51:13 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
05/01/2022 21:51:13 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
05/01/2022 21:51:13 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
05/01/2022 21:51:13 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

05/01/2022 21:51:16 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 21:51:16 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/01/2022 21:51:16 - INFO - root -   train data size: 1500, validation data size: 0
05/01/2022 21:51:16 - INFO - root -   Number of train optimization steps is : 279
05/01/2022 21:51:42 - INFO - root -   Epoch 1, Train loss : 0.0443
05/01/2022 21:52:08 - INFO - root -   Epoch 2, Train loss : 0.0030
05/01/2022 21:52:35 - INFO - root -   Epoch 3, Train loss : 0.0013
05/01/2022 21:52:57 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
05/01/2022 21:52:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
05/01/2022 21:52:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
05/01/2022 21:52:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

05/01/2022 21:53:00 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 21:53:00 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/01/2022 21:53:00 - INFO - root -   train data size: 2000, validation data size: 0
05/01/2022 21:53:00 - INFO - root -   Number of train optimization steps is : 375
05/01/2022 21:53:36 - INFO - root -   Epoch 1, Train loss : 0.0361
05/01/2022 21:54:10 - INFO - root -   Epoch 2, Train loss : 0.0027
05/01/2022 21:54:48 - INFO - root -   Epoch 3, Train loss : 0.0010
05/01/2022 21:55:11 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
05/01/2022 21:55:11 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
05/01/2022 21:55:11 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
05/01/2022 21:55:11 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

05/01/2022 21:55:13 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 21:55:13 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/01/2022 21:55:13 - INFO - root -   train data size: 2500, validation data size: 0
05/01/2022 21:55:13 - INFO - root -   Number of train optimization steps is : 468
05/01/2022 21:55:57 - INFO - root -   Epoch 1, Train loss : 0.0321
05/01/2022 21:56:41 - INFO - root -   Epoch 2, Train loss : 0.0026
05/01/2022 21:57:25 - INFO - root -   Epoch 3, Train loss : 0.0011
05/01/2022 21:57:48 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
05/01/2022 21:57:48 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
05/01/2022 21:57:48 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
05/01/2022 21:57:48 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

05/01/2022 21:57:52 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 21:57:52 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/01/2022 21:57:52 - INFO - root -   train data size: 3000, validation data size: 0
05/01/2022 21:57:52 - INFO - root -   Number of train optimization steps is : 561
05/01/2022 21:58:44 - INFO - root -   Epoch 1, Train loss : 0.0292
05/01/2022 21:59:37 - INFO - root -   Epoch 2, Train loss : 0.0023
05/01/2022 22:00:30 - INFO - root -   Epoch 3, Train loss : 0.0010
05/01/2022 22:00:54 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
05/01/2022 22:00:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
05/01/2022 22:00:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
05/01/2022 22:00:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

05/01/2022 22:00:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 22:00:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/01/2022 22:00:57 - INFO - root -   train data size: 3500, validation data size: 0
05/01/2022 22:00:57 - INFO - root -   Number of train optimization steps is : 654
05/01/2022 22:02:00 - INFO - root -   Epoch 1, Train loss : 0.0274
05/01/2022 22:03:01 - INFO - root -   Epoch 2, Train loss : 0.0025
05/01/2022 22:04:03 - INFO - root -   Epoch 3, Train loss : 0.0011
05/01/2022 22:04:26 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
05/01/2022 22:04:26 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
05/01/2022 22:04:26 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
05/01/2022 22:04:26 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

05/01/2022 22:04:29 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 22:04:29 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/01/2022 22:04:29 - INFO - root -   train data size: 4000, validation data size: 0
05/01/2022 22:04:29 - INFO - root -   Number of train optimization steps is : 750
05/01/2022 22:05:42 - INFO - root -   Epoch 1, Train loss : 0.0254
05/01/2022 22:06:52 - INFO - root -   Epoch 2, Train loss : 0.0024
05/01/2022 22:08:03 - INFO - root -   Epoch 3, Train loss : 0.0011
05/01/2022 22:08:26 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
05/01/2022 22:08:27 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
05/01/2022 22:08:27 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
05/01/2022 22:08:27 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

05/01/2022 22:08:29 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 22:08:29 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/01/2022 22:08:29 - INFO - root -   train data size: 4500, validation data size: 0
05/01/2022 22:08:29 - INFO - root -   Number of train optimization steps is : 843
05/01/2022 22:09:49 - INFO - root -   Epoch 1, Train loss : 0.0236
05/01/2022 22:11:08 - INFO - root -   Epoch 2, Train loss : 0.0021
05/01/2022 22:12:28 - INFO - root -   Epoch 3, Train loss : 0.0010
05/01/2022 22:12:51 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
05/01/2022 22:12:51 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
05/01/2022 22:12:51 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
05/01/2022 22:12:51 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

05/01/2022 22:12:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 22:12:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/01/2022 22:12:54 - INFO - root -   train data size: 5000, validation data size: 0
05/01/2022 22:12:54 - INFO - root -   Number of train optimization steps is : 936
05/01/2022 22:14:22 - INFO - root -   Epoch 1, Train loss : 0.0226
05/01/2022 22:15:50 - INFO - root -   Epoch 2, Train loss : 0.0021
05/01/2022 22:17:19 - INFO - root -   Epoch 3, Train loss : 0.0009
05/01/2022 22:17:42 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='scibert-scivocab-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
05/01/2022 22:17:42 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmphr2wn6s9
05/01/2022 22:17:52 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar from cache at /tmp/tmphr2wn6s9/scibert_scivocab_cased/vocab.txt
05/01/2022 22:17:52 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmpqxl25w3r
05/01/2022 22:18:02 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31116
}

05/01/2022 22:18:04 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 22:18:04 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
05/01/2022 22:18:04 - INFO - root -   train data size: 1000, validation data size: 0
05/01/2022 22:18:05 - INFO - root -   Number of train optimization steps is : 186
05/01/2022 22:18:23 - INFO - root -   Epoch 1, Train loss : 0.0375
05/01/2022 22:18:41 - INFO - root -   Epoch 2, Train loss : 0.0025
05/01/2022 22:18:59 - INFO - root -   Epoch 3, Train loss : 0.0007
05/01/2022 22:19:23 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmp9d45gfwm
05/01/2022 22:19:31 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar from cache at /tmp/tmp9d45gfwm/scibert_scivocab_cased/vocab.txt
05/01/2022 22:19:31 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmpr9bpwehj
05/01/2022 22:19:39 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31116
}

05/01/2022 22:19:42 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/01/2022 22:19:42 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
05/01/2022 22:19:42 - INFO - root -   train data size: 1500, validation data size: 0
05/01/2022 22:19:42 - INFO - root -   Number of train optimization steps is : 279
05/01/2022 22:20:09 - INFO - root -   Epoch 1, Train loss : 0.0288
05/01/2022 22:20:36 - INFO - root -   Epoch 2, Train loss : 0.0025
05/02/2022 12:13:20 - INFO - root -   Loading model:
BertTokenClassifier(bert_config_json={'architectures': ['BertForMaskedLM'],
                                      'attention_probs_dropout_prob': 0.1,
                                      'hidden_act': 'gelu',
                                      'hidden_dropout_prob': 0.1,
                                      'hidden_size': 768,
                                      'initializer_range': 0.02,
                                      'intermediate_size': 3072,
                                      'layer_norm_eps': 1e-12,
                                      'max_position_embeddings': 512,
                                      'model_type': 'bert',
                                      'num_attention_heads': 12,
                                      'num_hidden_layers': 12,
                                      'pad_t...
                                            ('[unused21]', 21),
                                            ('[unused22]', 22),
                                            ('[unused23]', 23),
                                            ('[unused24]', 24),
                                            ('[unused25]', 25),
                                            ('[unused26]', 26),
                                            ('[unused27]', 27),
                                            ('[unused28]', 28),
                                            ('[unused29]', 29), ...]),
                    do_lower_case=False, eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
05/02/2022 13:47:36 - WARNING - tensorflow -   Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer.iter
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer.decay
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer.momentum
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer.rho
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-0.embeddings
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.layer.kernel
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.layer.bias
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-1.forward_layer.cell.kernel
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-1.forward_layer.cell.recurrent_kernel
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-1.forward_layer.cell.bias
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-1.backward_layer.cell.kernel
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-1.backward_layer.cell.recurrent_kernel
05/02/2022 13:47:36 - WARNING - tensorflow -   Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-1.backward_layer.cell.bias
05/03/2022 05:48:59 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='scibert-scivocab-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
05/03/2022 05:49:00 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmp2rpr5zw8
05/03/2022 05:49:08 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar from cache at /tmp/tmp2rpr5zw8/scibert_scivocab_cased/vocab.txt
05/03/2022 05:49:09 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmpn5tvrvil
05/03/2022 05:49:17 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31116
}

05/03/2022 05:49:21 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/03/2022 05:49:21 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
05/03/2022 05:49:21 - INFO - root -   train data size: 100, validation data size: 0
05/03/2022 05:49:26 - INFO - root -   Number of train optimization steps is : 18
05/03/2022 05:49:28 - INFO - root -   Epoch 1, Train loss : 0.1681
05/03/2022 05:49:31 - INFO - root -   Epoch 2, Train loss : 0.0421
05/03/2022 05:49:33 - INFO - root -   Epoch 3, Train loss : 0.0180
05/03/2022 05:56:46 - INFO - root -   Loading model:
BertTokenClassifier(bert_model='scibert-scivocab-cased', eval_batch_size=16,
                    gradient_accumulation_steps=4, ignore_label=['O'],
                    label_list=['B', 'I', 'O'], learning_rate=5e-05,
                    max_seq_length=178, train_batch_size=16,
                    validation_fraction=0.0)
05/03/2022 05:56:51 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmpyo578ju8
05/03/2022 05:57:00 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar from cache at /tmp/tmpyo578ju8/scibert_scivocab_cased/vocab.txt
05/03/2022 05:57:00 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   extracting archive file /home/tuo96248/.cache/torch/pytorch_pretrained_bert/distributed_-1/97faa982a7622df6a24749b4433defe79d67f33fe93759ff23416ab0b6e139d9.33f020f8acb2b822e2dc2f8c16cb8a2aa2c62ba7ac2d21b555f1543691701ead to temp dir /tmp/tmpv9sc74oi
05/03/2022 05:57:09 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31116
}

05/03/2022 05:57:12 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertPlusMLP not initialized from pretrained model: ['mlp.weight', 'mlp.bias']
05/03/2022 05:57:12 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertPlusMLP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
05/03/2022 05:57:12 - INFO - root -   train data size: 157770, validation data size: 0
05/03/2022 05:57:17 - INFO - root -   Number of train optimization steps is : 29580
05/03/2022 06:45:41 - INFO - root -   Epoch 1, Train loss : 0.0059
05/03/2022 07:33:13 - INFO - root -   Epoch 2, Train loss : 0.0023
05/03/2022 08:21:40 - INFO - root -   Epoch 3, Train loss : 0.0014
