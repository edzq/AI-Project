{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "107cfe7a-7dd1-4289-860e-d4ecb721fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm.keras import TqdmCallback\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from nervaluate import Evaluator\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "230a0ae0-f6c3-40c4-85ce-cdb4d290a1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8456, 836)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Load train test split -----------\n",
    "# train_pids, valid_pids, test_pids, unseen_pids = pickle.load(open(\"train_test_split.p\", \"rb\"))\n",
    "train_pids, valid_pids, test_pids, test_pids_cat = pickle.load(open(\"train_test_split_0331.p\", \"rb\"))\n",
    "train_idxs, valid_idxs, test_idxs, unseen_idxs = [], [], [], []\n",
    "\n",
    "unseen_pids = test_pids_cat[\"unseen\"] + valid_pids.tolist()\n",
    "for k in test_pids_cat.keys():\n",
    "    if k == \"unseen\":\n",
    "        continue\n",
    "    unseen_pids = [i for i in unseen_pids if i not in test_pids_cat[k]]\n",
    "len(test_pids_cat[\"unseen\"]), len(unseen_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d59d51e6-6ba0-4982-ab25-b4ad57357aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../data/ml_datasetname_inputs_flv0.p\"\n",
    "X, y, X_pids = pickle.load(open(input_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "f683b4ac-05ea-40b1-988c-720dcd1cfd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b9d79385db4d7cbc2c6f51e5762348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nSamples: train=146,580 (69.74%), valid=21,217 (10.09%)\n",
      "test=42,388 (20.17%), unseen = 7,608 (3.62%)\n"
     ]
    }
   ],
   "source": [
    "for i in trange(len(X_pids)):\n",
    "    if X_pids[i] in train_pids:\n",
    "        train_idxs.append(i)\n",
    "    elif X_pids[i] in valid_pids:\n",
    "        valid_idxs.append(i)\n",
    "    elif X_pids[i] in test_pids:\n",
    "        test_idxs.append(i)\n",
    "    if X_pids[i] in unseen_pids:\n",
    "        unseen_idxs.append(i)\n",
    "        \n",
    "tot = len(train_idxs) + len(valid_idxs) + len(test_idxs)\n",
    "print(f\"nSamples: train={len(train_idxs):,} ({len(train_idxs)*100/tot:.2f}%), valid={len(valid_idxs):,} ({len(valid_idxs)*100/tot:.2f}%)\")\n",
    "print(f\"test={len(test_idxs):,} ({len(test_idxs)*100/tot:.2f}%), unseen = {len(unseen_idxs):,} ({len(unseen_idxs)*100/tot:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3e8b3103-6ec2-4b4c-a0c6-d541d29a9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- spliting -------------\n",
    "X_tr = np.array([X[i] for i in train_idxs])\n",
    "X_val = np.array([X[i] for i in valid_idxs])\n",
    "X_te = np.array([X[i] for i in test_idxs])\n",
    "X_te_seen = np.array([X[i] for i in test_idxs if i not in unseen_idxs])\n",
    "X_te_unseen = np.array([X[i] for i in test_idxs if i in unseen_idxs])\n",
    "\n",
    "y_tr = np.array([y[i] for i in train_idxs])\n",
    "y_val = np.array([y[i] for i in valid_idxs])\n",
    "y_te = np.array([y[i] for i in test_idxs])\n",
    "y_te_unseen = np.array([y[i] for i in test_idxs if i in unseen_idxs])\n",
    "y_te_seen = np.array([y[i] for i in test_idxs if i not in unseen_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ae8089-e8da-4ad1-8207-43102ec97b0f",
   "metadata": {},
   "source": [
    "# SciBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "506d64ea-34f3-43ab-bf47-3cf046401b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(os.getcwd() + \"/bert-sklearn/\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import statistics as stats\n",
    "\n",
    "from bert_sklearn import BertClassifier\n",
    "from bert_sklearn import BertRegressor\n",
    "from bert_sklearn import BertTokenClassifier\n",
    "from bert_sklearn import load_model\n",
    "from nervaluate import Evaluator\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "1b070bfc-6ee7-40c8-9846-f73722e34eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load inputs-----------\n",
    "X, y, X_pids = pickle.load(open(\"./data/ml_datasetname_inputs_flv0.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "9566e7de-4164-4244-a578-8feea6117746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- spliting -------------\n",
    "X_tr = [X[i] for i in train_idxs]\n",
    "X_val = [X[i] for i in valid_idxs]\n",
    "X_te = [X[i] for i in test_idxs]\n",
    "X_te_seen = [X[i] for i in test_idxs if i not in unseen_idxs]\n",
    "X_te_unseen = [X[i] for i in test_idxs if i in unseen_idxs]\n",
    "\n",
    "y_tr = [y[i] for i in train_idxs]\n",
    "y_val = [y[i] for i in valid_idxs]\n",
    "y_te = [y[i] for i in test_idxs]\n",
    "y_te_unseen = [y[i] for i in test_idxs if i in unseen_idxs]\n",
    "y_te_seen = [y[i] for i in test_idxs if i not in unseen_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "374c703d-46f2-4516-bad3-49cb5176f3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../ckpts/scibert.bin...\n",
      "Defaulting to linear classifier/regressor\n",
      "Building sklearn token classifier...\n"
     ]
    }
   ],
   "source": [
    "#savefile = '../data/scibert_Jo_split.bin'\n",
    "savefile = './checkpoints/scibert.bin'\n",
    "# # load model from disk\n",
    "model = load_model(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4ef9ac60-65d3-4080-9e87-3c9f7571285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|███████████████████████████████████████████████████████████████████████████████████| 2355/2355 [05:09<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.92      0.93      0.92     70044\n",
      "\n",
      "   micro avg       0.92      0.93      0.92     70044\n",
      "   macro avg       0.92      0.93      0.92     70044\n",
      "weighted avg       0.92      0.93      0.92     70044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(X_te_seen)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te_seen]\n",
    "print(\"-\"*10, \"Test Seen\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4c12ffa5-a463-4a82-ac0c-5f19481cf1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|███████████████████████████████████████████████████████████████████████████████████| 1327/1327 [02:54<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Valid ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.91      0.90      0.90     38680\n",
      "\n",
      "   micro avg       0.91      0.90      0.90     38680\n",
      "   macro avg       0.91      0.90      0.90     38680\n",
      "weighted avg       0.91      0.90      0.90     38680\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|███████████████████████████████████████████████████████████████████████████████████| 2650/2650 [05:48<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Test ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.91      0.91      0.91     79878\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     79878\n",
      "   macro avg       0.91      0.91      0.91     79878\n",
      "weighted avg       0.91      0.91      0.91     79878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(X_val)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_val]\n",
    "print(\"-\"*10, \"Valid\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "test_pred = model.predict(X_te)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te]\n",
    "print(\"-\"*10, \"Test\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b4c4216f-d14e-431c-8610-afce42c7e296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ent_type': {'correct': 73279,\n",
       "  'incorrect': 0,\n",
       "  'partial': 0,\n",
       "  'missed': 6599,\n",
       "  'spurious': 6286,\n",
       "  'possible': 79878,\n",
       "  'actual': 79565,\n",
       "  'precision': 0.920995412555772,\n",
       "  'recall': 0.9173865144345126,\n",
       "  'f1': 0.9191874212100877},\n",
       " 'partial': {'correct': 72484,\n",
       "  'incorrect': 0,\n",
       "  'partial': 795,\n",
       "  'missed': 6599,\n",
       "  'spurious': 6286,\n",
       "  'possible': 79878,\n",
       "  'actual': 79565,\n",
       "  'precision': 0.915999497266386,\n",
       "  'recall': 0.9124101755176645,\n",
       "  'f1': 0.9142013133220022},\n",
       " 'strict': {'correct': 72484,\n",
       "  'incorrect': 795,\n",
       "  'partial': 0,\n",
       "  'missed': 6599,\n",
       "  'spurious': 6286,\n",
       "  'possible': 79878,\n",
       "  'actual': 79565,\n",
       "  'precision': 0.9110035819769999,\n",
       "  'recall': 0.9074338366008162,\n",
       "  'f1': 0.9092152054339168},\n",
       " 'exact': {'correct': 72484,\n",
       "  'incorrect': 795,\n",
       "  'partial': 0,\n",
       "  'missed': 6599,\n",
       "  'spurious': 6286,\n",
       "  'possible': 79878,\n",
       "  'actual': 79565,\n",
       "  'precision': 0.9110035819769999,\n",
       "  'recall': 0.9074338366008162,\n",
       "  'f1': 0.9092152054339168}}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = Evaluator([list(i) for i in y_te], preds,\n",
    "                      tags=[\"\"], loader='list')\n",
    "results, results_per_tag = evaluator.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "11ddecdd-456d-4d06-b201-756a4fd38acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|█████████████████████████████████████████████████████████████████████████████████████| 295/295 [00:39<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Unseen ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.86      0.75      0.80      9834\n",
      "\n",
      "   micro avg       0.86      0.75      0.80      9834\n",
      "   macro avg       0.86      0.75      0.80      9834\n",
      "weighted avg       0.86      0.75      0.80      9834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(X_te_unseen)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te_unseen]\n",
    "print(\"-\"*10, \"Unseen\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0f228830-f59f-4248-bb6a-1bc70b4f5822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word           | gt| pr\n",
      "-------------------------\n",
      "are            | O | O \n",
      "with           | O | O \n",
      "dense          | O | O \n",
      "objects        | O | O \n",
      "in             | O | O \n",
      "TinyPerson     | B | B \n",
      ",              | O | O \n",
      "DETECTIONS     | O | O \n",
      "PER            | O | O \n",
      "IMG            | O | O \n"
     ]
    }
   ],
   "source": [
    "sample_id = 102\n",
    "print_results(X_te_unseen[sample_id], y_te_unseen[sample_id], preds[sample_id], print_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46397b18-9e04-429b-9318-211b6b0987c3",
   "metadata": {},
   "source": [
    "#  BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "7505ab96-9456-4980-a15e-d67d2aaeaa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../ckpts/bert_base.bin...\n",
      "Defaulting to linear classifier/regressor\n",
      "Building sklearn token classifier...\n"
     ]
    }
   ],
   "source": [
    "savefile = './checkpoints/bert_base.bin'\n",
    "# # load model from disk\n",
    "model = load_model(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "293ec278-a4a7-4506-92af-ea38fbf067ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenClassifier(bert_config_json={'architectures': ['BertForMaskedLM'],\n",
      "                                      'attention_probs_dropout_prob': 0.1,\n",
      "                                      'hidden_act': 'gelu',\n",
      "                                      'hidden_dropout_prob': 0.1,\n",
      "                                      'hidden_size': 768,\n",
      "                                      'initializer_range': 0.02,\n",
      "                                      'intermediate_size': 3072,\n",
      "                                      'layer_norm_eps': 1e-12,\n",
      "                                      'max_position_embeddings': 512,\n",
      "                                      'model_type': 'bert',\n",
      "                                      'num_attention_heads': 12,\n",
      "                                      'num_hidden_layers': 12,\n",
      "                                      'pad_t...\n",
      "                                            ('[unused21]', 21),\n",
      "                                            ('[unused22]', 22),\n",
      "                                            ('[unused23]', 23),\n",
      "                                            ('[unused24]', 24),\n",
      "                                            ('[unused25]', 25),\n",
      "                                            ('[unused26]', 26),\n",
      "                                            ('[unused27]', 27),\n",
      "                                            ('[unused28]', 28),\n",
      "                                            ('[unused29]', 29), ...]),\n",
      "                    do_lower_case=False, eval_batch_size=16,\n",
      "                    gradient_accumulation_steps=4, ignore_label=['O'],\n",
      "                    label_list=['B', 'I', 'O'], learning_rate=5e-05,\n",
      "                    max_seq_length=178, train_batch_size=16,\n",
      "                    validation_fraction=0.0)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d1771082-0ffd-4214-a28d-1f0062aefd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|███████████████████████████████████████████████████████████████████████████████████| 1327/1327 [02:55<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Valid ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.91      0.91      0.91     38680\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     38680\n",
      "   macro avg       0.91      0.91      0.91     38680\n",
      "weighted avg       0.91      0.91      0.91     38680\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|███████████████████████████████████████████████████████████████████████████████████| 2355/2355 [05:10<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Test Seen ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.92      0.93      0.92     70044\n",
      "\n",
      "   micro avg       0.92      0.93      0.92     70044\n",
      "   macro avg       0.92      0.93      0.92     70044\n",
      "weighted avg       0.92      0.93      0.92     70044\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|███████████████████████████████████████████████████████████████████████████████████| 2650/2650 [05:48<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Test ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.91      0.91      0.91     79878\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     79878\n",
      "   macro avg       0.91      0.91      0.91     79878\n",
      "weighted avg       0.91      0.91      0.91     79878\n",
      "\n",
      "{'ent_type': {'correct': 73591, 'incorrect': 0, 'partial': 0, 'missed': 6287, 'spurious': 6469, 'possible': 79878, 'actual': 80060, 'precision': 0.9191981014239321, 'recall': 0.9212924710183029, 'f1': 0.9202440945866525}, 'partial': {'correct': 72822, 'incorrect': 0, 'partial': 769, 'missed': 6287, 'spurious': 6469, 'possible': 79878, 'actual': 80060, 'precision': 0.9143954534099425, 'recall': 0.916478880292446, 'f1': 0.9154359814428091}, 'strict': {'correct': 72822, 'incorrect': 769, 'partial': 0, 'missed': 6287, 'spurious': 6469, 'possible': 79878, 'actual': 80060, 'precision': 0.909592805395953, 'recall': 0.9116652895665891, 'f1': 0.9106278682989658}, 'exact': {'correct': 72822, 'incorrect': 769, 'partial': 0, 'missed': 6287, 'spurious': 6469, 'possible': 79878, 'actual': 80060, 'precision': 0.909592805395953, 'recall': 0.9116652895665891, 'f1': 0.9106278682989658}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|█████████████████████████████████████████████████████████████████████████████████████| 295/295 [00:40<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Unseen ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.86      0.76      0.81      9834\n",
      "\n",
      "   micro avg       0.86      0.76      0.81      9834\n",
      "   macro avg       0.86      0.76      0.81      9834\n",
      "weighted avg       0.86      0.76      0.81      9834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(X_val)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_val]\n",
    "print(\"-\"*10, \"Valid\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "test_pred = model.predict(X_te_seen)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te_seen]\n",
    "print(\"-\"*10, \"Test Seen\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "test_pred = model.predict(X_te)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te]\n",
    "print(\"-\"*10, \"Test\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "evaluator = Evaluator([list(i) for i in y_te], preds,\n",
    "                      tags=[\"\"], loader='list')\n",
    "results, results_per_tag = evaluator.evaluate()\n",
    "print(results)\n",
    "\n",
    "test_pred = model.predict(X_te_unseen)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te_unseen]\n",
    "print(\"-\"*10, \"Unseen\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fa99c-ad74-44c4-a510-4a0c0cbde538",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 102\n",
    "print_results(X_te_unseen[sample_id], y_te_unseen[sample_id], preds[sample_id], print_all=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
