{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a29d53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm.keras import TqdmCallback\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from nervaluate import Evaluator\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17343957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8456, 836)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Load train test split -----------\n",
    "# train_pids, valid_pids, test_pids, unseen_pids = pickle.load(open(\"train_test_split.p\", \"rb\"))\n",
    "train_pids, valid_pids, test_pids, test_pids_cat = pickle.load(open(\"./data/train_test_split_0331.p\", \"rb\"))\n",
    "train_idxs, valid_idxs, test_idxs, unseen_idxs = [], [], [], []\n",
    "\n",
    "unseen_pids = test_pids_cat[\"unseen\"] + valid_pids.tolist()\n",
    "for k in test_pids_cat.keys():\n",
    "    if k == \"unseen\":\n",
    "        continue\n",
    "    unseen_pids = [i for i in unseen_pids if i not in test_pids_cat[k]]\n",
    "len(test_pids_cat[\"unseen\"]), len(unseen_pids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f731136",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b99ca486",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./data/ml_datasetname_inputs_flv0.p\"\n",
    "X, y, X_pids = pickle.load(open(input_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c8022c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656b5497a59347748cedd65917304438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nSamples: train=146,580 (69.74%), valid=21,217 (10.09%)\n",
      "test=42,388 (20.17%), unseen = 7,608 (3.62%)\n"
     ]
    }
   ],
   "source": [
    "for i in trange(len(X_pids)):\n",
    "    if X_pids[i] in train_pids:\n",
    "        train_idxs.append(i)\n",
    "    elif X_pids[i] in valid_pids:\n",
    "        valid_idxs.append(i)\n",
    "    elif X_pids[i] in test_pids:\n",
    "        test_idxs.append(i)\n",
    "    if X_pids[i] in unseen_pids:\n",
    "        unseen_idxs.append(i)\n",
    "        \n",
    "tot = len(train_idxs) + len(valid_idxs) + len(test_idxs)\n",
    "print(f\"nSamples: train={len(train_idxs):,} ({len(train_idxs)*100/tot:.2f}%), valid={len(valid_idxs):,} ({len(valid_idxs)*100/tot:.2f}%)\")\n",
    "print(f\"test={len(test_idxs):,} ({len(test_idxs)*100/tot:.2f}%), unseen = {len(unseen_idxs):,} ({len(unseen_idxs)*100/tot:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a868d1",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5d11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Preprocess inputs-----------\n",
    "word_to_ix = {}\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent in X:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "word_to_ix[\"ENDPAD\"] = len(word_to_ix) # the corresponding padding\n",
    "words = word_to_ix.keys()\n",
    "ix_to_word = dict((v, k) for k, v in word_to_ix.items())\n",
    "\n",
    "tag_to_ix = {\n",
    "'O': 0,\n",
    "'B': 1,\n",
    "'I': 2,\n",
    "}\n",
    "\n",
    "X = [[word_to_ix[w] for w in s] for s in X]\n",
    "y = [[to_categorical(tag_to_ix[w], num_classes=3) for w in s] for s in y]\n",
    "\n",
    "max_len = len(X[0])\n",
    "n_words = len(word_to_ix.keys())\n",
    "n_tags = len(tag_to_ix.keys())\n",
    "\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word_to_ix[\"ENDPAD\"])\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag_to_ix[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94e88e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-3f9124a99b14>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_tr = np.array([X[i] for i in train_idxs])\n",
      "<ipython-input-16-3f9124a99b14>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_val = np.array([X[i] for i in valid_idxs])\n",
      "<ipython-input-16-3f9124a99b14>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_te = np.array([X[i] for i in test_idxs])\n",
      "<ipython-input-16-3f9124a99b14>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_te_seen = np.array([X[i] for i in test_idxs if i not in unseen_idxs])\n",
      "<ipython-input-16-3f9124a99b14>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_te_unseen = np.array([X[i] for i in test_idxs if i in unseen_idxs])\n",
      "<ipython-input-16-3f9124a99b14>:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_tr = np.array([y[i] for i in train_idxs])\n",
      "<ipython-input-16-3f9124a99b14>:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_val = np.array([y[i] for i in valid_idxs])\n",
      "<ipython-input-16-3f9124a99b14>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_te = np.array([y[i] for i in test_idxs])\n",
      "<ipython-input-16-3f9124a99b14>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_te_unseen = np.array([y[i] for i in test_idxs if i in unseen_idxs])\n",
      "<ipython-input-16-3f9124a99b14>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_te_seen = np.array([y[i] for i in test_idxs if i not in unseen_idxs])\n"
     ]
    }
   ],
   "source": [
    "# ----------- spliting -------------\n",
    "X_tr = np.array([X[i] for i in train_idxs])\n",
    "X_val = np.array([X[i] for i in valid_idxs])\n",
    "X_te = np.array([X[i] for i in test_idxs])\n",
    "X_te_seen = np.array([X[i] for i in test_idxs if i not in unseen_idxs])\n",
    "X_te_unseen = np.array([X[i] for i in test_idxs if i in unseen_idxs])\n",
    "\n",
    "y_tr = np.array([y[i] for i in train_idxs])\n",
    "y_val = np.array([y[i] for i in valid_idxs])\n",
    "y_te = np.array([y[i] for i in test_idxs])\n",
    "y_te_unseen = np.array([y[i] for i in test_idxs if i in unseen_idxs])\n",
    "y_te_seen = np.array([y[i] for i in test_idxs if i not in unseen_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301146e7",
   "metadata": {},
   "source": [
    "# BiLSTM(without embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c674c2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x147f23406580>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=n_words, output_dim=300, input_length=max_len)(input_)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "model = Model(input_, out)\n",
    "model.load_weights('./checkpoints/BiLSTM_no_pretrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1e42dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664/664 [==============================] - 10s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.92      0.77      0.84     38680\n",
      "\n",
      "   micro avg       0.92      0.77      0.84     38680\n",
      "   macro avg       0.92      0.77      0.84     38680\n",
      "weighted avg       0.92      0.77      0.84     38680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test validation set\n",
    "valid_pred = model.predict(X_val, verbose=1)\n",
    "test_labels = pred2labels(y_val).tolist()\n",
    "pred_labels = pred2labels(valid_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58880a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178/1178 [==============================] - 17s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.93      0.81      0.87     70044\n",
      "\n",
      "   micro avg       0.93      0.81      0.87     70044\n",
      "   macro avg       0.93      0.81      0.87     70044\n",
      "weighted avg       0.93      0.81      0.87     70044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test seen set\n",
    "seen_pred = model.predict(X_te_seen, verbose=1)\n",
    "test_labels = pred2labels(y_te_seen).tolist()\n",
    "pred_labels = pred2labels(seen_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d243f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1325/1325 [==============================] - 19s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.92      0.78      0.85     79878\n",
      "\n",
      "   micro avg       0.92      0.78      0.85     79878\n",
      "   macro avg       0.92      0.78      0.85     79878\n",
      "weighted avg       0.92      0.78      0.85     79878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test all\n",
    "test_pred = model.predict(X_te, verbose=1)\n",
    "test_labels = pred2labels(y_te).tolist()\n",
    "pred_labels = pred2labels(test_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3dd4c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 2s 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.88      0.58      0.70      9834\n",
      "\n",
      "   micro avg       0.88      0.58      0.70      9834\n",
      "   macro avg       0.88      0.58      0.70      9834\n",
      "weighted avg       0.88      0.58      0.70      9834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test zero-shot \n",
    "unseen_pred = model.predict(X_te_unseen, verbose=1)\n",
    "test_labels = pred2labels(y_te_unseen).tolist()\n",
    "pred_labels = pred2labels(unseen_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01419fc6",
   "metadata": {},
   "source": [
    "## BiLSTM(Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88eb2ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x154df6b2f6a0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=n_words, output_dim=300, input_length=max_len)(input_)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "model = Model(input_, out)\n",
    "model.load_weights('./checkpoints/BiLSTM_Glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2073940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664/664 [==============================] - 10s 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.90      0.84      0.87     38680\n",
      "\n",
      "   micro avg       0.90      0.84      0.87     38680\n",
      "   macro avg       0.90      0.84      0.87     38680\n",
      "weighted avg       0.90      0.84      0.87     38680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test validation set\n",
    "valid_pred = model.predict(X_val, verbose=1)\n",
    "test_labels = pred2labels(y_val).tolist()\n",
    "pred_labels = pred2labels(valid_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f619932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178/1178 [==============================] - 17s 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.91      0.88      0.89     70044\n",
      "\n",
      "   micro avg       0.91      0.88      0.89     70044\n",
      "   macro avg       0.91      0.88      0.89     70044\n",
      "weighted avg       0.91      0.88      0.89     70044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test seen set\n",
    "seen_pred = model.predict(X_te_seen, verbose=1)\n",
    "test_labels = pred2labels(y_te_seen).tolist()\n",
    "pred_labels = pred2labels(seen_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cae0edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1325/1325 [==============================] - 19s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.90      0.85      0.87     79878\n",
      "\n",
      "   micro avg       0.90      0.85      0.87     79878\n",
      "   macro avg       0.90      0.85      0.87     79878\n",
      "weighted avg       0.90      0.85      0.87     79878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test all\n",
    "test_pred = model.predict(X_te, verbose=1)\n",
    "test_labels = pred2labels(y_te).tolist()\n",
    "pred_labels = pred2labels(test_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c121a801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 3s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.85      0.64      0.73      9834\n",
      "\n",
      "   micro avg       0.85      0.64      0.73      9834\n",
      "   macro avg       0.85      0.64      0.73      9834\n",
      "weighted avg       0.85      0.64      0.73      9834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test zero-shot \n",
    "unseen_pred = model.predict(X_te_unseen, verbose=1)\n",
    "test_labels = pred2labels(y_te_unseen).tolist()\n",
    "pred_labels = pred2labels(unseen_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a4bce",
   "metadata": {},
   "source": [
    "## BiLSTM(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e526441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x154df70d3070>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=n_words, output_dim=300, input_length=max_len)(input_)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "model = Model(input_, out)\n",
    "model.load_weights('./checkpoints/BiLSTM_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7c327fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664/664 [==============================] - 10s 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.91      0.85      0.88     38680\n",
      "\n",
      "   micro avg       0.91      0.85      0.88     38680\n",
      "   macro avg       0.91      0.85      0.88     38680\n",
      "weighted avg       0.91      0.85      0.88     38680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test validation set\n",
    "valid_pred = model.predict(X_val, verbose=1)\n",
    "test_labels = pred2labels(y_val).tolist()\n",
    "pred_labels = pred2labels(valid_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55bb956f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178/1178 [==============================] - 17s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.91      0.89      0.90     70044\n",
      "\n",
      "   micro avg       0.91      0.89      0.90     70044\n",
      "   macro avg       0.91      0.89      0.90     70044\n",
      "weighted avg       0.91      0.89      0.90     70044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test seen set\n",
    "seen_pred = model.predict(X_te_seen, verbose=1)\n",
    "test_labels = pred2labels(y_te_seen).tolist()\n",
    "pred_labels = pred2labels(seen_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba2e6500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1325/1325 [==============================] - 19s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.91      0.86      0.88     79878\n",
      "\n",
      "   micro avg       0.91      0.86      0.88     79878\n",
      "   macro avg       0.91      0.86      0.88     79878\n",
      "weighted avg       0.91      0.86      0.88     79878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test all\n",
    "test_pred = model.predict(X_te, verbose=1)\n",
    "test_labels = pred2labels(y_te).tolist()\n",
    "pred_labels = pred2labels(test_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a3213eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 2s 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.89      0.65      0.75      9834\n",
      "\n",
      "   micro avg       0.89      0.65      0.75      9834\n",
      "   macro avg       0.89      0.65      0.75      9834\n",
      "weighted avg       0.89      0.65      0.75      9834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test zero-shot \n",
    "unseen_pred = model.predict(X_te_unseen, verbose=1)\n",
    "test_labels = pred2labels(y_te_unseen).tolist()\n",
    "pred_labels = pred2labels(unseen_pred).tolist()\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea3b8f",
   "metadata": {},
   "source": [
    "# CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc5d3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed data\n",
    "X = pickle.load(open(\"./data/CRF_X.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d388d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-3f9124a99b14>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_tr = np.array([X[i] for i in train_idxs])\n",
      "<ipython-input-55-3f9124a99b14>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_val = np.array([X[i] for i in valid_idxs])\n",
      "<ipython-input-55-3f9124a99b14>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_te = np.array([X[i] for i in test_idxs])\n",
      "<ipython-input-55-3f9124a99b14>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_te_seen = np.array([X[i] for i in test_idxs if i not in unseen_idxs])\n",
      "<ipython-input-55-3f9124a99b14>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_te_unseen = np.array([X[i] for i in test_idxs if i in unseen_idxs])\n",
      "<ipython-input-55-3f9124a99b14>:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_tr = np.array([y[i] for i in train_idxs])\n",
      "<ipython-input-55-3f9124a99b14>:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_val = np.array([y[i] for i in valid_idxs])\n",
      "<ipython-input-55-3f9124a99b14>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_te = np.array([y[i] for i in test_idxs])\n",
      "<ipython-input-55-3f9124a99b14>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_te_unseen = np.array([y[i] for i in test_idxs if i in unseen_idxs])\n",
      "<ipython-input-55-3f9124a99b14>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_te_seen = np.array([y[i] for i in test_idxs if i not in unseen_idxs])\n"
     ]
    }
   ],
   "source": [
    "# ----------- spliting -------------\n",
    "X_tr = np.array([X[i] for i in train_idxs])\n",
    "X_val = np.array([X[i] for i in valid_idxs])\n",
    "X_te = np.array([X[i] for i in test_idxs])\n",
    "X_te_seen = np.array([X[i] for i in test_idxs if i not in unseen_idxs])\n",
    "X_te_unseen = np.array([X[i] for i in test_idxs if i in unseen_idxs])\n",
    "\n",
    "y_tr = np.array([y[i] for i in train_idxs])\n",
    "y_val = np.array([y[i] for i in valid_idxs])\n",
    "y_te = np.array([y[i] for i in test_idxs])\n",
    "y_te_unseen = np.array([y[i] for i in test_idxs if i in unseen_idxs])\n",
    "y_te_seen = np.array([y[i] for i in test_idxs if i not in unseen_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff1fcfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "crf = CRF(algorithm='lbfgs',\n",
    "          c1=10,\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=False)\n",
    "filename = 'crf_Qi_v2.pkl'\n",
    "crf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8bd423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained CRF model\n",
    "# filename = 'crf_Qi_v2.pkl'\n",
    "# crf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de192324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.90      0.83      0.86     79878\n",
      "\n",
      "   micro avg       0.90      0.83      0.86     79878\n",
      "   macro avg       0.90      0.83      0.86     79878\n",
      "weighted avg       0.90      0.83      0.86     79878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = crf.predict(X_te)\n",
    "print(classification_report([list(i) for i in y_te], test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c16e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Valid ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.90      0.81      0.85     38680\n",
      "\n",
      "   micro avg       0.90      0.81      0.85     38680\n",
      "   macro avg       0.90      0.81      0.85     38680\n",
      "weighted avg       0.90      0.81      0.85     38680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "test_pred = crf.predict(X_val)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_val]\n",
    "print(\"-\"*10, \"Valid\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12b66481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Unseen ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.89      0.63      0.73      9834\n",
      "\n",
      "   micro avg       0.89      0.63      0.73      9834\n",
      "   macro avg       0.89      0.63      0.73      9834\n",
      "weighted avg       0.89      0.63      0.73      9834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test for unseen\n",
    "test_pred = crf.predict(X_te_unseen)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te_unseen]\n",
    "print(\"-\"*10, \"Unseen\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ac17187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Seen ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.90      0.86      0.88     70044\n",
      "\n",
      "   micro avg       0.90      0.86      0.88     70044\n",
      "   macro avg       0.90      0.86      0.88     70044\n",
      "weighted avg       0.90      0.86      0.88     70044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test for seen\n",
    "test_pred = crf.predict(X_te_seen)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te_seen]\n",
    "print(\"-\"*10, \"Seen\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e189e81",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8125e981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 12:13:15 bert_sklearn.model.pytorch_pretrained.modeling INFO: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./checkpoints/bert_base.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 12:13:18 bert_sklearn.model.pytorch_pretrained.modeling INFO: Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Building sklearn token classifier...\n"
     ]
    }
   ],
   "source": [
    "from bert_sklearn import load_model\n",
    "savefile = './checkpoints/bert_base.bin'\n",
    "model = load_model(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58d70105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1327/1327 [01:23<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Valid ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.91      0.93      0.92     38680\n",
      "\n",
      "   micro avg       0.91      0.93      0.92     38680\n",
      "   macro avg       0.91      0.93      0.92     38680\n",
      "weighted avg       0.91      0.93      0.92     38680\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 2355/2355 [02:29<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Test Seen ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.97      1.00      0.98     70044\n",
      "\n",
      "   micro avg       0.97      1.00      0.98     70044\n",
      "   macro avg       0.97      1.00      0.98     70044\n",
      "weighted avg       0.97      1.00      0.98     70044\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 2650/2650 [02:46<00:00, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Test ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.97      0.99      0.98     79878\n",
      "\n",
      "   micro avg       0.97      0.99      0.98     79878\n",
      "   macro avg       0.97      0.99      0.98     79878\n",
      "weighted avg       0.97      0.99      0.98     79878\n",
      "\n",
      "{'ent_type': {'correct': 79510, 'incorrect': 0, 'partial': 0, 'missed': 368, 'spurious': 1987, 'possible': 79878, 'actual': 81497, 'precision': 0.9756187344319422, 'recall': 0.9953929742857858, 'f1': 0.9854066615027111}, 'partial': {'correct': 79437, 'incorrect': 0, 'partial': 73, 'missed': 368, 'spurious': 1987, 'possible': 79878, 'actual': 81497, 'precision': 0.9751708651852216, 'recall': 0.9949360274418488, 'f1': 0.9849542989930288}, 'strict': {'correct': 79437, 'incorrect': 73, 'partial': 0, 'missed': 368, 'spurious': 1987, 'possible': 79878, 'actual': 81497, 'precision': 0.9747229959385008, 'recall': 0.9944790805979118, 'f1': 0.9845019364833463}, 'exact': {'correct': 79437, 'incorrect': 73, 'partial': 0, 'missed': 368, 'spurious': 1987, 'possible': 79878, 'actual': 81497, 'precision': 0.9747229959385008, 'recall': 0.9944790805979118, 'f1': 0.9845019364833463}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 295/295 [00:19<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Unseen ----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.98      0.99      0.99      9834\n",
      "\n",
      "   micro avg       0.98      0.99      0.99      9834\n",
      "   macro avg       0.98      0.99      0.99      9834\n",
      "weighted avg       0.98      0.99      0.99      9834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(X_val)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_val]\n",
    "print(\"-\"*10, \"Valid\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "test_pred = model.predict(X_te_seen)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te_seen]\n",
    "print(\"-\"*10, \"Test Seen\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "test_pred = model.predict(X_te)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te]\n",
    "print(\"-\"*10, \"Test\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "evaluator = Evaluator([list(i) for i in y_te], preds,\n",
    "                      tags=[\"\"], loader='list')\n",
    "results, results_per_tag = evaluator.evaluate()\n",
    "print(results)\n",
    "\n",
    "test_pred = model.predict(X_te_unseen)\n",
    "preds = [[j if j is not None else 'O' for j in i] for i in test_pred]\n",
    "test_labels = [np.array(i).astype('<U1').tolist() for i in y_te_unseen]\n",
    "print(\"-\"*10, \"Unseen\", \"-\"*10)\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05221ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
