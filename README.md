# AI-Project

# To-Do

Train the models:

**Transfer learning**
- Sci-BERT(Qi finished)
- BERT
- BERT-CRF

**Popular sequence labeling models**
- CRF
- BiLSTM
- BiLSTM-CRF

# 1.Object

Extract the dataset name of literatures.

# 2.Dataset 

Dataset I generated by merging the paperswithcode and S2ROC.
https://drive.google.com/file/d/1wvd6J4sxJLtZfBj4JxHREW5mQUPuZiVt/view?usp=sharing

When using this dataset, you can do:

```
import pickle
X, y, X_pids = pickle.load(open("./data/ml_datasetname_inputs_flv0.p", "rb"))
```


X is the token, y is the BIO-label. Ignore the X_pids. The X_pid is the paper_id.

Then we can split the X and y to training and testing dataset

# 3.Reference github code and Paper

Paper: https://www.mdpi.com/2306-5729/6/8/84

The code: https://github.com/xjaeh/ner_dataset_recognition

So the model we at least plan to run(all of them can be found in the github in the above link):
CRF
Sci-BERT(Qi already finished the training)
BERT
BiLSTM
BiLSTM-CRF

And then we would like to try the models:
BERT-CRF


# 4.Computing environment

Temple HPC:https://www.hpc.temple.edu/

Suggest you to apply the access.

## How to use the romte jupyter notebook by HPC

See https://docs.anaconda.com/anaconda/user-guide/tasks/remote-jupyter-notebook/

1.connect the ssh of hpc

ssh xxxx@dgx-1.hpc.temple.edu

2.activate the conda environment

source ~/.bashrc

3.run jupyternote book without browser

jupyter notebook --no-browser --port=8080

4.local terminal run ssh

ssh -L 8080:localhost:8080 tuoxxxx@dgx-1.hpc.temple.edu

5.open chrome, input the adress!

# 5. Experiment

## Experiment log



## Evluation methods:

https://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/



